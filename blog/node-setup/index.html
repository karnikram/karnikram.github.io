<!DOCTYPE html>
<style>
    @media only screen and (max-device-width: 420px){
        .navcompact{
            width:100%;
        }
    }
        
    @media only screen and (min-device-width: 421px){
       .navcompact{
           width:60%;
       }
   } 
</style>

<html lang="en-us">
	<head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="author" content="Karnik Ram">

<meta name="generator" content="Hugo 0.80.0" />
<title>Setting up a server node</title>
<link rel="shortcut icon" href="https://karnikram.info/images/favicon.ico">
<link rel="stylesheet" href="https://karnikram.info/css/style.css">
<link rel="stylesheet" href="https://karnikram.info/css/highlight.css">




<meta property="og:title" content="Setting up a server node" />
<meta property="og:description" content="A summary of the procedure involved in setting up a typical server node in a cluster" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://karnikram.info/blog/node-setup/" />
<meta property="article:published_time" content="2020-06-28T18:58:10+05:30" />
<meta property="article:modified_time" content="2020-06-28T18:58:10+05:30" />


<meta itemprop="name" content="Setting up a server node">
<meta itemprop="description" content="A summary of the procedure involved in setting up a typical server node in a cluster">
<meta itemprop="datePublished" content="2020-06-28T18:58:10+05:30" />
<meta itemprop="dateModified" content="2020-06-28T18:58:10+05:30" />
<meta itemprop="wordCount" content="2633">



<meta itemprop="keywords" content="" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Setting up a server node"/>
<meta name="twitter:description" content="A summary of the procedure involved in setting up a typical server node in a cluster"/>

    </head>
<body>
    <table class="navcompact" border="0" align="center" cellspacing="9px" cellpadding="0">
		<tr>
			<td>
					<nav class="main-nav">
	
		
			<a href='https://karnikram.info/blog'> <span class="arrow">‚Üê</span> Blog </a>
			&nbsp;
		
		<a href='https://karnikram.info'> About Me</a>
	

	
</nav>
			</td>
		</tr>
		</table>
    <section id="wrapper">
        
        
        
<article class="post">
    <header>
        <h1>Setting up a server node</h1>
        <h2 class="headline">
        June 28, 2020
        <br>
        
        </h2>
    </header>
    <section id="post-body">
        <p>The following is something that I wrote as part of our internal documentation for our lab, but I decided to share it here because I&rsquo;ve discussed some neat tools like Environment Modules and Prometheus among others that I think more people will find useful. Another reason is because I haven&rsquo;t posted anything in a while (hashtag content). And just to be clear, the hostnames and IPs referenced here will be unreachable as they are internal to our network.</p>
<hr>
<p>This document details the process involved in setting up a new compute node, and adding it to the RRC compute cluster. This can be used as reference for sysadmins and users who want to understand how things are setup.</p>
<p>We assume that all the required hardware is properly installed and discuss only the software setup. We begin with a fresh installation of Ubuntu server 18.04.4 LTS.</p>
<br>
<h2 id="basic-working-environment">Basic working environment</h2>
<p>We begin by logging in as the root user. Root login using password is disabled by default, and only works with key-based authentication. So we need to place our public SSH key at <code>/root/.ssh/authorized_keys</code> on the server node beforehand. Once inside the root environment, we begin by installing our favourite cli tools:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apt update
apt install vim tmux htop iotop nload sysstat git python-pip python3-pip zsh corkscrew
</code></pre></div><p>We then set zsh to be our default shell, and install prezto as our zsh manager. We also setup the network proxy environment variables, to avoid using transparent proxy. To do all this we run the following script:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -L https://researchweb.iiit.ac.in/~srisai.poonganam/setup/shell-setup.zsh | zsh -
</code></pre></div><p>As part of the shell setup we also add the following lines to the system-wide <code>/etc/bash.bashrc</code> file.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Force logout idle sessions</span>
<span style="color:#66d9ef">if</span> <span style="color:#f92672">[[</span> -n $SSH_CONNECTION <span style="color:#f92672">]]</span> ; <span style="color:#66d9ef">then</span>
        <span style="color:#75715e"># Close idle user session after 2 hours</span>
        export TMOUT<span style="color:#f92672">=</span><span style="color:#ae81ff">7200</span>
<span style="color:#66d9ef">fi</span>

<span style="color:#75715e"># Query and show home quota usage from black (data server)</span>
echo <span style="color:#e6db74">&#34;&#34;</span>
curl -s black.iiit.ac.in:8080/$USER 2&gt; /dev/null
echo <span style="color:#e6db74">&#34;&#34;</span>
</code></pre></div><br>
<h2 id="home-directories-mount">Home directories mount</h2>
<p>All the home directories of our users are stored centrally on the black server, under the <code>/mnt/rrc/home</code> directory. This directory comes with data guarantees, and is where we expect users to store their code, models, and any other data that they want to persist. This directory is exported by black to all the compute nodes using NFS.
To allow our node to access this export, we add the following line (with the node IP) to the <code>/etc/exports</code> file on black:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">/mnt/rrc/home 10.4.18.9<span style="color:#f92672">(</span>rw,async,no_root_squash,no_subtree_check<span style="color:#f92672">)</span>
</code></pre></div><p>We then make sure the new contents are read by the NFS daemon on black by running <code>exportfs -a</code>.</p>
<p>On our node (client side), we run the following commands to manually mount the exported home directories:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apt install nfs-common
mount -v -t nfs 10.4.18.3:/mnt/rrc/home /home
</code></pre></div><p>To enable automatic mounting (on boot), we add the following line to <code>/etc/fstab:</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">10.4.18.3:/mnt/rrc/home /home nfs rw,soft,intr <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">0</span>
</code></pre></div><p>At this point the local <code>/home</code> directory on our node should contain all the user directories fetched from black. This centralised setup allows users to easily move between different nodes, and allows us to manage daily backups and quotas from a single location. But since this is mounted over the network, there will be latency and so we expect users to use this directory only as a cold long-term storage. For local operations we setup a scratch drive.</p>
<br>
<h2 id="scratch-space-mount">Scratch space mount</h2>
<p>Each compute node has attached to it a 1 TB SSD drive (sometimes 2 SSDs configured as a RAID0 drive) meant as low-latency storage for users to work with locally for their I/O operations. To mount this drive, we run the following commands (with the right device file name):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir /scratch
mount /dev/sdb1 /scratch
</code></pre></div><p>In addition, as done for the home space mount, we add the following line to <code>/etc/fstab</code> (run <code>blkid</code> to identify the right UUID):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">/dev/disk/by-uuid/b63e7654-3442-4eb7-98d4-25e78618d235 /scratch ext4 defaults <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">0</span>
</code></pre></div><p>Since this is a drive shared by all the users connected to the node, we setup <code>quota</code> to prevent any user from occupying too much space that would restrict other users. To enable quota for /scratch, we first modify the <code>/etc/fstab</code> file with the <code>usrquota</code> option as follows:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">/dev/disk/by-uuid/b63e7654-3442-4eb7-98d4-25e78618d235 /scratch ext4 defaults,usrquota <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">0</span>
</code></pre></div><p>Then we remount the drive and create the quota index and quota files using the following commands:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apt install quota
mount -o remount /scratch
quotacheck -cum /scratch
quotaon /scratch
</code></pre></div><p>Though quota is enabled for the drive now, we allocate quotas for the users only when the disk usage exceeds a certain threshold. This is explained in the <code>Custom Scripts</code> section.</p>
<br>
<h2 id="software-packages">Software packages</h2>
<p>Apart from the basic CLI tools already installed in the first step, we try not to install anything using apt install. Installing any package often brings with it its own version of system dependencies which can silently break someone else&rsquo;s workflow. These dependencies can also get auto-updated during OS upgrades (if enabled). So we expect users to build any packages that they need locally by themselves, or install from conda-forge if available. That said, we build some commonly used packages from source by ourselves and install them to <code>/opt/</code>. This includes CUDA &amp; cuDNN, Singularity, gcc, and Matlab. However we don&rsquo;t always need to build all these tools. Most of these are statically built, so we can just copy all their files from the <code>/opt</code> directory of another compute node onto this node and things will work just fine.</p>
<br>
<h2 id="environment-modules">Environment Modules</h2>
<p>We often need to build and manage different versions of the same tool (eg: CUDA), as users work on different projects and have different requirements. The hard way to manage them is to manually change one&rsquo;s PATH variable to point to the right version of the tool each time. The easier way is to use the <a href="%5Bhttps://modules.readthedocs.io/en/latest/index.html%5D(https://modules.readthedocs.io/en/latest/index.html)">Environment Modules</a> package. Modules provides a clean interface for dynamically modifying one&rsquo;s environment variables using modulefiles. This allows easy loading of different versions of a tool or library without having to manually change the PATH variable each time.
To install Modules we run the following commands:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apt install automake autopoint tclsh tcl8.6-dev
mkdir -p /opt/Modules/modulefiles
./configure --prefix<span style="color:#f92672">=</span>/opt/Modules <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>	--modulefilesdir<span style="color:#f92672">=</span>/opt/Modules/modulefiles
make <span style="color:#f92672">&amp;&amp;</span> make install
</code></pre></div><p>Then we add the following lines to <code>/etc/bash.bashrc</code> and <code>/etc/zsh/zshrc</code> , respectively, to make Modules available at shell startup:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">source /opt/Modules/init/bash
source /opt/Modules/init/zsh
</code></pre></div><p>Finally, we add modulefiles for all the tools that we want to make available as &lsquo;modules&rsquo; into the <code>/opt/Modules/modulefiles</code> directory. The following is an example modulefile for CUDA 10.2, saved as <code>/opt/Modules/modulefiles/10.2</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#%Module1.0</span>
<span style="color:#75715e">##</span>
<span style="color:#75715e">## CUDA Modulefile</span>
<span style="color:#75715e">##</span>
proc ModulesHelp <span style="color:#f92672">{</span> <span style="color:#f92672">}</span> <span style="color:#f92672">{</span>
    global version

    puts stderr <span style="color:#e6db74">&#34;\n\tThis module adds CUDA-10.2 to your environment variable&#34;</span>
    puts stderr <span style="color:#e6db74">&#34;\tDirectory: </span>$root<span style="color:#e6db74">&#34;</span>
<span style="color:#f92672">}</span>

module-whatis   <span style="color:#e6db74">&#34;adds CUDA-10.2 to your environment variable&#34;</span>

set           version          10.2
set           app              CUDA
set           root             /opt/cuda/10.2
prepend-path   PATH             $root/bin
prepend-path   LD_LIBRARY_PATH  $root/lib64

</code></pre></div><p>With this file added, CUDA 10.2 can be loaded dynamically using <code>module load cuda/10.2</code>. These modulefiles can be directly copied from any other node.</p>
<br>
<h2 id="hostname">Hostname</h2>
<p>Before we be begin connecting our node to other services, we fix the hostname. Our convention is to name a node after any uniquely identifiable color on its hardware (such imagination). We name this node <code>blue</code>. To fix this, we add an A record on our Cloudflare DNS, to point the IP address of the node to <code>blue</code>. The full hostname would be <code>blue.rrcx.ml</code>, where <code>rrcx.ml</code> is our domain managed on Cloudflare. We have another domain <code>rrc.iiit.ac.in</code> which resides on the IIIT nameservers. We don&rsquo;t have direct control on this domain, but it is already made to point to <code>rrcx.ml</code> using a DNAME record. So, <code>blue.rrc.iiit.ac.in</code> is another valid hostname for this node and is what we expect most users to use. If needed, we can also request the university server room to add another CNAME record to point this to <code>blue.iiit.ac.in</code>.</p>
<br>
<h2 id="custom-scripts">Custom scripts</h2>
<p>At this point we start cloning some of our maintenance scripts from our Github repo. The repository contains many maintenance scripts, but the ones relevant for this compute node are,</p>
<p>-<code>clear-storage.sh</code>: Deletes files in the <code>/scratch</code> directory that haven&rsquo;t been accessed in two weeks. The respective owners are also sent a notification one day before.</p>
<p>-<code>quota-bot.sh</code>: Checks the usage of <code>/scratch</code> directory and if it is greater than 85% of capacity, a limit of 375 GB is set for every user whose usage is greater than 150 GB.</p>
<p>-<code>daily-mails.sh</code>: Reports files in <code>/tmp</code> that are not owned by root.</p>
<p>We use SSH to connect to our Github repositories. We have a bot account, named rrc-iith, that has access to all the repositories on our RoboticsIIITH organization page. We generate public and private keys for our node using the following command and add the public key to the bot account on Github for access to all the repositories from this node.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh-keygen -o -a <span style="color:#ae81ff">100</span> -t ed25519 -f ~/.ssh/id_ed25519
</code></pre></div><p>SSH connections to Github (or any external server) however are blocked within the IIIT network. Fortunately, Github supports SSH connections over the HTTPS port to the ssh.github.com server. To force all connections to Github to run through this server and port, we add the following lines to the <code>~/.ssh/config</code> file:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">Host github.com
	User rrc-iiith
	Port <span style="color:#ae81ff">443</span>
	Hostname ssh.github.com
	TCPKeepAlive yes
	IdentitiesOnly yes
	ProxyCommand corkscrew proxy.iiit.ac.in <span style="color:#ae81ff">8080</span> %h %p
</code></pre></div><p>Further, we modify our &lsquo;/root/.gitconfig&rsquo; to identify the hostname as follows:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git config --global user.name <span style="color:#e6db74">&#39;rrc-bot&#39;</span> user.email <span style="color:#e6db74">&#39;root@blue.rrc.iiit.ac.in&#39;</span>
</code></pre></div><p>Once the connection is setup we clone our <code>sysad-scripts</code> repository from our Github page onto the root directory <code>/root/sysad-scripts</code>. We also create another directory <code>scripts</code> as a symlink to this. We then create a crontab (<code>crontab.blue</code>) to automatically run these scripts at certain times as well as pull updates, if any, from the repo.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">
<span style="color:#ae81ff">0</span> */3 * * * root cd /root/sysad-scripts <span style="color:#f92672">&amp;&amp;</span> /usr/bin/git pull &gt; /dev/null <span style="color:#75715e"># runs every three hours</span>
<span style="color:#ae81ff">0</span> <span style="color:#ae81ff">3</span> * * * root /root/scripts/clear-storage/clear-storage.sh <span style="color:#75715e"># runs at 3 AM every day</span>
<span style="color:#ae81ff">0</span> <span style="color:#ae81ff">0</span> * * * root /root/scripts/daily-mails.sh
<span style="color:#ae81ff">0</span> * * * * root /root/scripts/quota-bot.sh

</code></pre></div><p>This is then symlinked to <code>/etc/cron.d/sysad-scripts</code> where it&rsquo;s picked up by cron and run accordingly.</p>
<br>
<h2 id="monitoring-tools">Monitoring tools</h2>
<p>We use Prometheus for system health monitoring. Prometheus is running on the server at <code>10.4.18.8:9090</code>, and it scrapes and stores time-series data using metrics exposed by the OS. We use <code>node_exporter</code> for exposing OS and CPU related metrics to Prometheus, and <code>nvidia_gpu_prometheus_exporter</code> for GPU related metrics. Both of these are static binaries which can directly be copied from another compute node to <code>/opt/metrics/</code>. We also copy the respective service files to <code>/etc/systems/system/metrics-exporter.service</code> and <code>/etc/systemd/system/gpu-metrics.exporter.service</code>. Then we run the following commands,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">systemctl start metrics-exporter.service
systemctl start gpu-metrics.exporter.service
</code></pre></div><p>At this point the exporters would now be exposing the metrics for Prometheus. To verify, we run the following commands:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl http://blue.iiit.ac.in:9100/metrics
curl http://blue.iiit.ac.in:9445/metrics
</code></pre></div><p>Then, to access these metrics on the Prometheus server, we edit the file <code>/opt/prometheus/prometheus.yml</code> to include the following:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">- targets: <span style="color:#f92672">[</span><span style="color:#e6db74">&#39;blue.iiit.ac.in:9100&#39;</span><span style="color:#f92672">]</span>
      labels:
        group: <span style="color:#e6db74">&#39;compute_nodes&#39;</span>
        hostname: blue.rrc.iiit.ac.in
        app: node_exporter

- targets: <span style="color:#f92672">[</span><span style="color:#e6db74">&#39;blue.iiit.ac.in:9445&#39;</span><span style="color:#f92672">]</span>
      labels:
        group: <span style="color:#e6db74">&#39;gpu_nodes&#39;</span>
        hostname: blue.rrc.iiit.ac.in
        app: nvidia_gpu_prometheus_exporter
</code></pre></div><p>Finally, we run <code>systemctl restart prometheus.service</code> to start scraping the data. We use Grafana for visualizing the scraped metrics from Prometheus, and creating alerts. Grafana is setup on the same server as Prometheus and is available at monitor.rrc.iiit.ac.in. A new dashboard with panels, and alert rules need to be created for this node. These can be directly copied from the dashboard of another node and are not discussed here.</p>
<div id="images">
<figure>
<a href="/images/grafana-node-exporter.png"><img src="/images/grafana-node-exporter.png" alt="grafana-node-exporter.png" width="400"/></a>
<figcaption> Grafana dashboard for the `node_exporter` metrics</figcaption>
</figure>
</div>
<br>
<h2 id="etc-backup">/etc backup</h2>
<p>The <code>/etc</code> directory contains many important system configuration files that need to be kept track of. We use etckeeper to keep track of these files. It stores all the files inside a git repository and automatically commits any changes made to these files. It can also push these changes to a remote repository for backups. The setup is simple:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apt install etckeeper
cd /etc
etckeeper init
</code></pre></div><p>To sync it to a remote repository, we create a <em>private</em> repository (blue-etckeeper) on the RoboticsIIITH organization page and give administrative access to the SysAds team. Then we run <code>git remote add origin git@github.com:RoboticsIIITH/blue-etckeeper.git</code> inside the <code>/etc</code> folder, and edit the <code>PUSH_REMOTE</code> option in <code>/etc/etckeeper/etckeeper.conf</code> with the name of the remote - <code>origin</code>. The SSH config for Github needs to be properly setup for this to work, as described earlier.</p>
<br>
<h2 id="disable-auto-upgrades">Disable auto-upgrades</h2>
<p>The apt package manager periodically updates the package lists to . However, we never intend to install any of these updates as they can easily break someone&rsquo;s workflow. So to disable this we set <code>APT::Periodic::Update-Package-Lists</code> to <code>0</code> in the file <code>/etc/apt/apt.conf.d/20-autoupgrades</code>.</p>
<p>Moreover, apt also tends to automatically apply any critical security updates. But most of these are updates at the kernel level which require a reboot, so we decided not to install any of these security updates too (!). To disable this we set <code>APT:Periodic::Unattended-Upgrade</code> to <code>0</code> in the same file.</p>
<br>
<h2 id="ipa-enrolment">IPA enrolment</h2>
<p>We use a FreeIPA server for user and host identity management, and authentication. To enrol this node as a host to the IPA server we run the following commands:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apt install freeipa-client
ipa-client-install -v <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>	--mkhomedir <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>	--ntp-server<span style="color:#f92672">=</span>time.iiit.ac.in <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>	--fixed-primary <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>	--hostname<span style="color:#f92672">=</span>blue.rrc.iiit.ac.in
</code></pre></div><p>Running this installer will require admin IPA credentials, and assumes that the host SSH keys have already been created. Our IPA server is running at <code>[ipa.rrc.iiit.ac.in](http://ipa.rrc.iiit.ac.in)</code> and the installer will find it by default by using the domain name <code>rrc.iiit.ac.in</code> that is parent to the hostname. The <code>--mkhomedir</code> is used to configure the host to create a user&rsquo;s home directory upon login if it does not exist yet. The <code>--fixed-primary</code> option specifies that the host will use this server as a fixed server even during failures and will not attempt to fallback to a different server.</p>
<p>Once the node is enrolled as a host, it should appear under the <code>Identity‚ÜíHosts</code> tab (use the web ui of the IPA server).</p>
<br>
<h2 id="firewall">Firewall</h2>
<p>As a final step before we enable user access, we setup a basic firewall for the node. We use <code>iptables</code> for this. <code>iptables</code> applies chains of rules to incoming network packets to determine what to do with matching packets. We first run the following commands to setup our table,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># flush existing rules</span>
iptables -F INPUT
iptables -F FORWARD
iptables -F OUTPUT

<span style="color:#75715e"># default targets</span>
iptables -P INPUT ACCEPT
iptables -P FORWARD DROP
iptables -P OUTPUT ACCEPT
</code></pre></div><p>Next, we specify which ports to accept connections from.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">iptables -A INPUT -p tcp -m tcp --dport <span style="color:#ae81ff">80</span> -j ACCEPT <span style="color:#75715e">#http</span>
iptables -A INPUT -p tcp -m tcp --dport <span style="color:#ae81ff">443</span> -j ACCEPT <span style="color:#75715e">#https</span>
iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport <span style="color:#ae81ff">22</span> -j ACCEPT <span style="color:#75715e">#ssh</span>
iptables -A INPUT -p tcp -m tcp --dport <span style="color:#ae81ff">3389</span> -j ACCEPT <span style="color:#75715e">#rdp</span>
iptables -A INPUT -p tcp -m tcp --dport <span style="color:#ae81ff">9445</span> -j ACCEPT <span style="color:#75715e">#node_exporter</span>
iptables -A INPUT -p tcp -m tcp --dport <span style="color:#ae81ff">9100</span> -j ACCEPT <span style="color:#75715e">#gpu_node_exporter</span>
</code></pre></div><p>Finally, we add known source IPs and reject everything else.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">iptables -A INPUT -s 10.4.18.2/32 -j ACCEPT <span style="color:#75715e">#green</span>
iptables -A INPUT -s 10.4.18.3/32 -j ACCEPT <span style="color:#75715e">#black</span>
iptables -A INPUT -s 10.4.18.4/32 -j ACCEPT <span style="color:#75715e">#neon</span>
iptables -A INPUT -p icmp -j ACCEPT 
iptables -A INPUT -s 127.0.0.0/8 -j ACCEPT
iptables -A INPUT -s 10.4.18.7/32 -p tcp -m state --state NEW -m tcp --dport <span style="color:#ae81ff">5666</span> -j ACCEPT <span style="color:#75715e">#not sure?</span>
iptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
iptables -A INPUT -j REJECT --reject-with icmp-port-unreachable
</code></pre></div><br>
<p>Run <code>iptables -L -v</code> to ensure if it is setup as expected.</p>
<h2 id="enable-user-access">Enable user access</h2>
<p>Finally, to enable users (also enrolled on the IPA server) to access this node, we create a new users group (eg: blue) under the <code>Identity ‚Üí Groups</code> tab of web IPA, and add the users who&rsquo;d like to have access to this node to this group.</p>
<p>Then we go to the <code>Policy ‚Üí Host-based Access Control</code> tab and create a new rule (eg: access_blue). Here we specify the name of the users group and the hostname they will get access to, and the default services.</p>
<p>At this point the users should be able to login to the node and start running their programs.</p>

    </section>
</article>

<footer id="post-meta" class="clearfix">
    
    <img class="avatar" src="https://karnikram.info/images/avatar.png">
    <div>
        <span class="dark">Karnik Ram</span>
        <span>I try.</span>
    </div>
    
    <section id="sharing">
        <a class="twitter" href="https://twitter.com/intent/tweet?text=https%3a%2f%2fkarnikram.info%2fblog%2fnode-setup%2f - Setting%20up%20a%20server%20node "><span class="icon-twitter"> Tweet</span></a>

<a class="facebook" href="#" onclick="
    window.open(
      'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
      'facebook-share-dialog',
      'width=626,height=436');
    return false;"><span class="icon-facebook-rect"> Share</span>
</a>
    </section>
</footer>

<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "karnikram-info" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


        <footer id="footer" style="margin-top:100px;margin-bottom:100px;">
        
</footer>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"/>
</script>
    </section>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
<script src="https://karnikram.info/js/main.js"></script>
<script src="https://karnikram.info/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>





</body>
</html>